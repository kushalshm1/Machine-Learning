{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will be learning what are Support Vector Machines, Why they are useful, What is the mathematics behind them and many more things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some prerequisites to learn before knowing about Support Vector Machines, let's discuss those first.\n",
    "\n",
    "## Prerequisites\n",
    "### Vectors:\n",
    "A vector is an Mathematical Object that can be represented by an Arrow which contains information such as magnitude of an entity as well as its direction in which it is pointed to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A vector as an Arrow](img/vector.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude of a Vector:\n",
    "Length of a Vector is also called Magnitude. \n",
    "It defines its strength\n",
    "Magnitude is also called Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direction of a Vector:\n",
    "Direction is the second component of a vector\n",
    "By Definition,\n",
    "Direction is a new vector for which the coordinates are the initial coordinates of our vector divided by magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let there be a vector and its coordinates are:\n",
    "$x = $($u,$v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then Direction of this vector x will be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Direction Vector Coordinates](img/directionVector.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product of Vectors:\n",
    "It is an operation performed on two vectors that returns a number. A number is sometimes called scalar, that's why dot product is also called Scalar product.\n",
    "\n",
    "![Scalar/Dot Product](img/dotProduct.jpg)\n",
    "\n",
    "θ is the angle between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplane:\n",
    "We use hyperplane to seperate data in Support Vector Machines when there are more than three dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line vs. Hyperplane\n",
    " \n",
    " Equation of Line:\n",
    "  ![Equation of Line](img/lineEquation.jpg)\n",
    "  \n",
    "  Equation of Hyperplane:\n",
    "  \n",
    "  ![Equation of Line](img/hyperplaneEquation.jpg)\n",
    "  \n",
    "  here m and c are both vectors of n dimension\n",
    "  \n",
    "  \n",
    "  And in Linear Hard margin Support Vector Machines we seperate the data by two seperating hyperplane each of which is at outermost point of each class as well as their distance is maximum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now starting Support Vector Machines classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two hyperplanes:\n",
    "![](img/SVMHyperplanes.jpg/)\n",
    "\n",
    "R is the perpendicular distance between two hyperplanes which is called Margin of both the hyperplanes.\n",
    "\n",
    "And this margin is computed as:\n",
    "\n",
    "![](img/SVMMarginDerivationPartial.jpg)\n",
    "\n",
    "\n",
    "R is the margin which is to be maximized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### There are some assumptions:\n",
    "![](img/PrimalProblemModified.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence our Primal Objective Function is this which is written above and we have to minimize it using mathematical optimization, But that is not an easy task to minimize it there are many more things to learn and do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's connect this problem to a real world problem\n",
    "We may encounter some situation when we have to classify between Male and Female voices and now we are going to use Support Vector Machines Algorithm to classify it.\n",
    "\n",
    "The two hyperplanes we made in the beginning are actually seperating our data with a bit of margin and there is a classification boundary between those margins which is also the decision boundary alternative of which there are male voices and female voices,\n",
    "you will better understand it using this Graphic,\n",
    " ![Hyperplances Seperating Male Female Voices](img/SVMHyperplanesWithData.jpg)\n",
    " \n",
    " \n",
    " Hence, we will be finding that minimum distance between hyperplanes at which the data gets seperated by each of the hyperplanes along with making a no man's land in the between which is further divided by a middle decision boundary and that is the very accurate classification of the data,\n",
    " and this was Support Vector Algorithm in a Nutshell,\n",
    " Now, let's see how will we do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Recap in case you don't know about Optimization\n",
    "\n",
    "### What is Optimization?\n",
    "Optimization is the branch of Mathematics and Research in which we deal with problems of Minimizing and Maximizing function in a feasible way.\n",
    "### Optimization Problem:\n",
    "An Optimization problem contains a function with some constraints which acts like bounding conditions of the value over whose extent that function will be minimized or maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this above problem of Male/Female voice classification we have formulated an Optimization Problem in which we have to minimize the margin function and our constraints are equations of Hyperplane.\n",
    "\n",
    "Hence, our optimization problem is,\n",
    "                \n",
    "![](img/PrimalProblem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After minimizing it we will get that minimum margin that classifies our data properly.\n",
    "\n",
    "This above written expression is our Primal problem\n",
    "#### But minimizing this Primal Problem is not easy so we will convert it into some other optimization problem that is easy to solve due to its resolved constraints and that Optimization problem is called Dual Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality:\n",
    "In order to make Dual Problem let's understand what is Duality first,\n",
    "\n",
    "So, in mathematical optimization theory, duality means that Optimization problems may be viewed from either of two perspectives, the Primal Problem and Dual Problem.\n",
    "\n",
    "### Duality Gap:\n",
    "![Dual and Primal Curves](img/DualAndPrimalCurves.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, imagine that in our primal problem we are trying to minimize the function at the top of the graph. Its minimum is point P.\n",
    "\n",
    "If we search for Dual Function we could end up with the one at bottom of the graph whose maximum is point D.\n",
    "\n",
    "We define the value (P-D) and call it Duality Gap.\n",
    "\n",
    "and,\n",
    "\n",
    "if(P-D)>0 then the phenomenon is called as Weak Duality\n",
    "\n",
    "if(P-D) = 0 then the phenomenon is called Strong Duality\n",
    "\n",
    "Now, we will see method of Lagrange Multipliers to find Dual Problem, but before that some prerequisite knowledge is required so let's review those first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Knowledge to learn Lagrange Multipliers Method\n",
    "\n",
    "### Gradients:\n",
    "A gradient of a function is a Vector field with the arrow pointing in the direction where the function is increasing.\n",
    "It is actually a partial derivative of function with respect to the variables.\n",
    "And it is always perpendicular to the curve of the function.\n",
    "\n",
    "let's take a sample optimization problem,\n",
    "![](img/SampleProblem.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these two equations f(x,y) and g(x,y)\n",
    "![](img/IndividualContourPlots.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine both the curves:\n",
    "![](img/CombinedContour.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets compute gradient of f(x,y) and g(x,y) both and let's see what happens:\n",
    "![](img/CombinedContoursWithGradients.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see red arrows are gradients of f(x,y) and black arrows are gradients of g(x,y).\n",
    "And among all these arrow there exists some red and black arrows which are parallel to each other.\n",
    "\n",
    "And here comes a line which shares a parallel gradient with circle,\n",
    "![](img/ParallelGradients.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the graph above we plot the line y = 1-x on the top of Objective  function f(x,y).\n",
    "\n",
    "### Lagrange found that minimum of f(x,y) under the constraint g(x,y) = 0 is obtained when their gradients point in the same direction.\n",
    "\n",
    "#### VERY IMPORTANT LINE:\n",
    "The point where two Gradient become parallel is the minimum of that objective function with respect to gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's translate it Mathematically:\n",
    "When two gradients are parallel but not equal so there should exist a parameter that should make both equal in magnitude, so that is λ(Lambda) and this is called the famous LAGRANGE MULTIPLIER.\n",
    "\n",
    "And hence the equation becomes, \n",
    "\n",
    "\n",
    "\n",
    "Gradient of (f(x,y)) = λ * (Gradient of g(x,y))\n",
    "\n",
    "or \n",
    "\n",
    "Δ(f(x,y)) = λ * Δ(g(x,y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is λ?\n",
    "It is what we call a Lagrange Multiplier.\n",
    "Indeed, even it the two gradient vectors point in the same direction, they might not have the same length, there must be some factor λ allowing to transform on the the other.\n",
    "\n",
    "But how do we find points for which,\n",
    "\n",
    "Gradient of (f(x,y)) = λ * (Gradient of g(x,y)) exists,\n",
    "\n",
    "To get this we need to modify this a bit,\n",
    "\n",
    "Δ(f(x,y)) - λ * Δ(g(x,y)) = 0\n",
    "\n",
    "To make things little easier we are defining a function:\n",
    "\n",
    "![](img/LagrangeFunction.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/LagrangeFunctionZero.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SVM Lagrangian problem:\n",
    "\n",
    "Now, we know what is a Lagrangian function, let's incorporate that in Support Vector Machines Algorithm.\n",
    "\n",
    "Till now our optimization problem is,\n",
    "![](img/PrimalProblem.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce our Lagrangian Function,\n",
    "![](img/LagrangeAfterValues.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now let's rewrite the problem using Duality Principle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/PrimalAndDual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to minimize w.r.t (w,b) and to maximize w.r.t (λ) at same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure of solving this type of problem\n",
    "\n",
    "In order to solve this problem we need to calculate Gradients of Lagrange function with respect to variables that are \"w\" and \"b\" here.\n",
    "\n",
    "Hence, our Gradients are,\n",
    "![](img/GradientsLagrange.jpg)\n",
    "\n",
    "![](img/ValueOfW.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's substitute this value of W in Lagrangian Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replacing value of W the function will remain only in terms of \"λ\" and \"b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/NewLagrangianLambdaAndB.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Further, we got,\n",
    "![](img/LagrangeATermGotZero.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we can see that our function only remains in terms of λ only, hence it became,\n",
    "![](img/NewLagrangeOnlyLambda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends only and only on Lagrange Multiplier,\n",
    "### Now, let's rewrite our Optimization Problem\n",
    "![](img/NewOptimizationProblemOnlyLambda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, in order to have a Guarrenteed solution we need to verify some conditions and those are called Karush-Kuhn-Tucker Conditions\n",
    "If a Solution Satisfies all the KKT Conditions then we can say by guarrentee that, this is Optimal Solution.\n",
    "\n",
    "### KKT Conditions are:\n",
    "1. Stationary Condition:\n",
    "![](img/KKTConditionOne.jpg)\n",
    "2. Primal Feasibility:\n",
    "![](img/KKTConditionTwo.jpg)\n",
    "3. Dual-Feasibility Condition:\n",
    "![](img/KKTConditionThree.jpg)\n",
    "4. Complementary Slackness Condition:\n",
    "![](img/KKTConditionFour.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this Fourth Condition(Complementary Slackness) we see that,\n",
    "\n",
    "![](img/KKTConditionFourFurther.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence, we can conclude that, Support Vectors are examples having Positive Lagrange Multipliers(λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hence, by optimization we need to find those Examples in Dataset for which Lagrange Multiplier is Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains explanation of SVM Algorithm only, there will be seperate notebook for its Programming in Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
